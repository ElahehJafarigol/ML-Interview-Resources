{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **üè° Must-Know ML Models for Coding Interviews (Housing Price Prediction)**  \n",
        "To **crush ML coding interviews**, you need to know how to **build, tune, and explain** key machine learning models. Here‚Äôs a list of models to **master**, grouped by category."
      ],
      "metadata": {
        "id": "vEhIEbAs-S7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üìå Linear & Generalized Models**\n",
        "‚úÖ **Used For:** Simple, interpretable, and fast models for regression & classification.\n",
        "\n",
        "| Model | Use Case | Key Concept |\n",
        "|-------|----------|------------|\n",
        "| **Linear Regression** | Regression | Fit a line to predict continuous values |\n",
        "| **Logistic Regression** | Classification | Predicts probability for binary/multi-class |\n",
        "| **Ridge & Lasso Regression** | Regression | Prevents overfitting via regularization |\n",
        "| **Polynomial Regression** | Regression | Fits curved relationships by adding polynomial terms |\n",
        "\n",
        "\n",
        "**üìå 1. Model Description & Key Equations**  \n",
        "\n",
        "Linear models **predict a target variable (y) as a weighted sum of input features (X)**. These are **fast, interpretable**, and **serve as a baseline** for regression and classification tasks.  \n",
        "\n",
        "**‚úÖ 1.1 Linear Regression**\n",
        "\n",
        "**Definition**\n",
        "\n",
        "Linear Regression models the relationship between the **independent variables** $ X $ and the **dependent variable** $ y $ by fitting a linear equation:\n",
        "\n",
        "$\n",
        "y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\epsilon\n",
        "$\n",
        "\n",
        "Where:  \n",
        "- $ y $ = target variable  \n",
        "- $ w_0 $ = **intercept** (bias)  \n",
        "- $ w_i $ = **weights** (coefficients for each feature)  \n",
        "- $ x_i $ = **input features**  \n",
        "- $ \\epsilon $ = **error term (residuals)**  \n",
        "\n",
        "**Cost Function: Mean Squared Error (MSE)**\n",
        "\n",
        "The goal is to minimize the error using **Mean Squared Error (MSE)**:\n",
        "\n",
        "$\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ y_i $ is the actual value\n",
        "- $ \\hat{y}_i $ is the predicted value\n",
        "\n",
        "**Solution Using the Normal Equation**\n",
        "\n",
        "Instead of using Gradient Descent, we can solve for **optimal weights** using the **closed-form solution**:\n",
        "\n",
        "$\n",
        "W = (X^TX)^{-1}X^Ty\n",
        "$\n",
        "\n",
        "**Gradient Descent Update Rule**\n",
        "\n",
        "If we use **Gradient Descent** to minimize MSE:\n",
        "\n",
        "$\n",
        "W := W - \\alpha \\frac{\\partial}{\\partial W} MSE\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ \\alpha $ = learning rate  \n",
        "- $ \\frac{\\partial}{\\partial W} MSE = \\frac{1}{n} X^T (XW - y) $  \n",
        "\n",
        "Thus, the update step:\n",
        "\n",
        "$\n",
        "W := W - \\alpha \\left( \\frac{1}{n} X^T (XW - y) \\right)\n",
        "$\n",
        "\n",
        "---\n",
        "\n",
        "**‚úÖ 1.2 Logistic Regression**\n",
        "\n",
        "**Definition**\n",
        "\n",
        "Logistic Regression is used for **classification problems**, where the output is **binary (0 or 1)**. Instead of fitting a straight line, we use the **Sigmoid function** to model probabilities:\n",
        "\n",
        "$\n",
        "P(y=1|X) = \\sigma(WX) = \\frac{1}{1 + e^{-WX}}\n",
        "$\n",
        "\n",
        "**Sigmoid Function**\n",
        "\n",
        "The Sigmoid function ensures the output is always between **0 and 1**:\n",
        "\n",
        "$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ z = WX $ (linear transformation)\n",
        "- The output can be interpreted as a probability.\n",
        "\n",
        "**Binary Cross-Entropy Loss (Log Loss)**\n",
        "\n",
        "To train Logistic Regression, we minimize the **log loss function**:\n",
        "\n",
        "$\n",
        "J(W) = - \\frac{1}{n} \\sum_{i=1}^{n} \\Big[y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\Big]\n",
        "$\n",
        "\n",
        "**Gradient Descent Update Rule**\n",
        "\n",
        "The weight update formula:\n",
        "\n",
        "$\n",
        "W := W - \\alpha \\sum (y_i - \\hat{y}_i) X\n",
        "$\n"
      ],
      "metadata": {
        "id": "z0EFs-ZgABg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå 2. Implementation from Scratch**\n",
        "\n",
        "**Preprocesing Titanic Dataset**\n"
      ],
      "metadata": {
        "id": "OYFHoezxCp6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vox18BHQXCpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df[\"Age\"] = df.groupby(\"Pclass\")[\"Age\"].transform(lambda x: x.fillna(x.median()))\n",
        "df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
        "df.drop(columns=[\"Cabin\", \"Ticket\", \"Name\"], inplace=True)\n",
        "\n",
        "# New variables\n",
        "df[\"family_size\"] = df[\"SibSp\"] + df[\"Parch\"]\n",
        "df.drop(columns=[\"SibSp\", \"Parch\"], inplace=True)\n",
        "\n",
        "# Transformations\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Categorical features\n",
        "cat_features = [\"Sex\", \"Embarked\"]\n",
        "encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
        "encoded_cat = encoder.fit_transform(df[cat_features]) # Encoded array\n",
        "encoded_df = pd.DataFrame(encoded_cat, columns=encoder.get_feature_names_out(cat_features))\n",
        "\n",
        "df = df.drop(columns=cat_features).reset_index(drop=True)\n",
        "df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "# Numerical variables\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df[\"Age\"] = scaler.fit_transform(df[[\"Age\"]])\n",
        "df[\"Fare\"] = scaler.fit_transform(df[[\"Fare\"]])\n",
        "df.head()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = df[\"Survived\"]\n",
        "X = df.drop(columns=[\"Survived\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3KDPEiThYJZF"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚úÖ 2.1 Linear Regression from Scratch**"
      ],
      "metadata": {
        "id": "f0k4No4Nailv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegressionScratch:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights =  np.random.randn(n_features) * 0.01 # Small random initialization\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Clip gradients to prevent exploding updates\n",
        "            dw = np.clip(dw, -1, 1)\n",
        "            db = np.clip(db, -1, 1)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def mean_squared_error(self, y_true, y_pred):\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Example Usage\n",
        "model = LinearRegressionScratch(learning_rate=0.01, epochs=1000)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "mse = model.mean_squared_error(y_test, predictions)\n",
        "print(f\"Test MSE: {mse: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MYQoi1qdunI",
        "outputId": "6378cc08-de0c-4e3b-b401-c45e8e83f29e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE:  14.9032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚úÖ 2.1 Logistic Regression from Scratch**"
      ],
      "metadata": {
        "id": "YYUuAmZ_irfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionScratch:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, num_features = X.shape\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self.sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Clip gradients to prevent exploding updates\n",
        "            dw = np.clip(dw, -1, 1)\n",
        "            db = np.clip(db, -1, 1)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        return [1 if i > 0.5 else 0 for i in y_pred]\n",
        "\n",
        "    def mean_squared_error(self, y_true, y_pred):\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "\n",
        "model = LogisticRegressionScratch(learning_rate=0.01, epochs=1000)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "mse = model.mean_squared_error(y_test, predictions)\n",
        "print(f\"Test MSE: {mse: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L2ZFBZMixVU",
        "outputId": "5d531228-6afe-4ea7-a5be-60ffe654c329"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE:  0.2402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå 3. Implementation Using Scikit-Learn**\n",
        "\n",
        "**‚úÖ 3.1 Linear Regression**"
      ],
      "metadata": {
        "id": "c4JJZc5IDnwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse =  mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Weights:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R¬≤ Score: {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtEb-hR5iyeT",
        "outputId": "9110f8c5-7382-44d3-fc31-cf919ef7c70d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [ 5.33635547e-05 -1.60214818e-01 -6.52287682e-02  1.66703503e-02\n",
            " -3.21229694e-02 -5.16026394e-01 -3.43881898e-02 -7.08167083e-02\n",
            "  7.55906450e-02]\n",
            "Intercept: 1.147936230738654\n",
            "Accuracy: 0.44094704973092147\n",
            "Mean Squared Error: 0.14\n",
            "R¬≤ Score: 0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚úÖ 3.2 Logistic Regression**"
      ],
      "metadata": {
        "id": "uWOMvMUAi2YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Weights:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R¬≤ Score: {r2:.2f}\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRYb1R7dpbPE",
        "outputId": "3374db37-5bee-4676-c7f9-1f38c6759917"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [[ 9.93558112e-04 -9.56267886e-01 -4.91594040e-01  1.74584005e-01\n",
            "  -3.01232587e-01 -2.70718792e+00  5.86553611e-02  1.35428296e-01\n",
            "   1.06047602e-01]]\n",
            "Intercept: [3.04575565]\n",
            "Accuracy: 0.8156424581005587\n",
            "Mean Squared Error: 0.18\n",
            "R¬≤ Score: 0.24\n",
            "\n",
            "Confusion Matrix:\n",
            " [[93 12]\n",
            " [21 53]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       105\n",
            "           1       0.82      0.72      0.76        74\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.82      0.80      0.81       179\n",
            "weighted avg       0.82      0.82      0.81       179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå 4. Advanced Tips & Interview Essentials**\n",
        "\n",
        "**üöÄ When to Use Linear vs. Logistic Regression**\n",
        "\n",
        "- Linear Regression ‚Üí Regression problems where features have a linear relationship.\n",
        "- Logistic Regression ‚Üí Classification problems where you need probability outputs.\n",
        "\n",
        "**üöÄ Common Pitfalls & Fixes**\n",
        "\n",
        "| **Mistake** | **Fix** |\n",
        "|------------|--------|\n",
        "| Forgetting to scale features in Linear Regression | Use `StandardScaler()` |\n",
        "| Using Linear Regression on categorical outcomes | Use Logistic Regression instead |\n",
        "| Using too many features in Logistic Regression | Apply Feature Selection (Lasso, PCA) |\n",
        "| Interpreting Logistic Regression coefficients directly | Convert them using Odds Ratio: `exp(coef_)` |\n"
      ],
      "metadata": {
        "id": "BOQm2_GrDuJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üî• Deep Dive into Ridge & Lasso Regression (Regularized Linear Models)**\n",
        "\n",
        "Ridge and Lasso Regression are regularized versions of Linear Regression that help prevent overfitting and improve generalization by adding penalties to the loss function.\n",
        "\n",
        "**üìå 1. Why Do We Need Ridge & Lasso Regression?**\n",
        "\n",
        "**üöÄ Problem: Overfitting in Linear Regression**\n",
        "In Linear Regression, we try to minimize the Mean Squared Error (MSE):\n",
        "\n",
        "$\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$\n",
        "\n",
        "However, when we have:\n",
        "‚úÖ Too many features  \n",
        "‚úÖ Highly correlated features\n",
        "‚úÖ Noise in the dataset  \n",
        "\n",
        "The model memorizes the training data instead of generalizing to new data.  \n",
        "üëâ **Solution?** We introduce regularization to control the size of coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "**üìå 2. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "**üîπ How It Works**\n",
        "Ridge Regression adds an L2 penalty to the loss function, preventing the weights from growing too large:\n",
        "\n",
        "$\n",
        "Loss = MSE + \\lambda \\sum w_i^2\n",
        "$\n",
        "\n",
        "- The second term $ \\lambda \\sum w_i^2 $ is the L2 penalty.\n",
        "- Large values of $ \\lambda $ ‚Üí More regularization (weights shrink).\n",
        "- Small values of $ \\lambda $ ‚Üí Regularization has little effect (behaves like normal Linear Regression).\n",
        "\n",
        "üëâ **Ridge keeps all features but shrinks coefficients towards zero.**  \n",
        "üëâ **Useful when all features contribute a little bit to the prediction.**\n",
        "\n",
        "**‚úÖ Ridge Regression Formula (Normal Equation)**\n",
        "\n",
        "Instead of directly minimizing MSE, Ridge Regression minimizes:\n",
        "\n",
        "$\n",
        "W = (X^TX + \\lambda I)^{-1}X^Ty\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ I $ is the identity matrix\n",
        "- $ \\lambda $ controls the amount of regularization.\n",
        "\n",
        "**üî• Ridge Regression in Python**"
      ],
      "metadata": {
        "id": "FVupXtUuOKzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Train ridge regression\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Ridge Coefficients:\", ridge.coef_)\n",
        "print(\"Intercept:\", ridge.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnBl26e-ABV2",
        "outputId": "365c2a95-8e6d-453b-c814-66472c66d7c4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Coefficients: [ 5.31406377e-05 -1.60034029e-01 -6.50475118e-02  1.70223836e-02\n",
            " -3.19818872e-02 -5.12866312e-01 -3.31954395e-02 -7.04694629e-02\n",
            "  5.17993067e-02]\n",
            "Intercept: 1.145121122441591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå 3. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "**üîπ How It Works**\n",
        "\n",
        "Lasso Regression adds an L1 penalty, which encourages sparsity:\n",
        "\n",
        "$\n",
        "Loss = MSE + \\lambda \\sum |w_i|\n",
        "$\n",
        "\n",
        "- The second term $ \\lambda \\sum |w_i| $ is the L1 penalty.\n",
        "- Lasso sets some coefficients to exactly zero (feature selection).\n",
        "- Large values of $ \\lambda $ ‚Üí More regularization (more coefficients shrink to 0).\n",
        "- Small values of $ \\lambda $ ‚Üí Less regularization (more features retained).\n",
        "\n",
        "üëâ **Lasso selects only the most important features and drops the rest.**  \n",
        "üëâ **Useful when some features are irrelevant or redundant.**\n",
        "\n",
        "üëâ **If a coefficient is 0, the corresponding feature has been dropped!**  \n",
        "üëâ **Lasso performs feature selection automatically!**\n",
        "\n",
        "**üî• Lasso Regression in Python**"
      ],
      "metadata": {
        "id": "QjxEnVzNSHCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Train Lasso regression\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Print selected features\n",
        "print(\"Lasso Coefficients:\", lasso.coef_)\n",
        "print(\"Intercept:\", lasso.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYyJd6b9SWFy",
        "outputId": "ab9316e9-fb0c-4985-954a-7afa171fbb43"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Coefficients: [ 3.97404265e-05 -3.36672266e-02 -0.00000000e+00  4.16069729e-05\n",
            " -0.00000000e+00 -1.02836591e-01  0.00000000e+00 -0.00000000e+00\n",
            "  0.00000000e+00]\n",
            "Intercept: 0.504488076321894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå 4. Comparing Ridge & Lasso**\n",
      "metadata": {
        "id": "mDa0QDhlO0pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "lasso = Lasso(alpha=0.1) # Higher alpha = more feature elimination\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "selected_features = X_train.columns[lasso.coef_ != 0]\n",
        "print(\"Selected Features:\", selected_features)\n",
        "\n",
        "ridge = Ridge(alpha=0.8) # Higher alpha = more regularization\n",
        "ridge.fit(X_train, y_train)\n",
        "print(\"Ridge weights:\\n\", ridge.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1WQ-GNWTE5w",
        "outputId": "e3c7b6aa-db6b-4dcb-a552-8297ca0b8d82"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features: Index(['PassengerId', 'Pclass', 'Fare', 'Sex_male'], dtype='object')\n",
            "Ridge weights:\n",
            " [ 5.31845301e-05 -1.60073507e-01 -6.50783416e-02  1.69515085e-02\n",
            " -3.20104455e-02 -5.13505760e-01 -3.34458829e-02 -7.05564189e-02\n",
            "  5.51726211e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå 5. Hyperparameter Tuning for Ridge & Lasso**\n",
        "\n",
        "Since $ \\lambda $ (alpha) controls the amount of regularization, we tune it using GridSearchCV:"
      ],
      "metadata": {
        "id": "22xGV7gOe0SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\"alpha\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Grid search for Ridge\n",
        "ridge_grid = GridSearchCV(Ridge(), param_grid, cv=5, scoring=\"r2\")\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "print(\"Best Alpha for Ridge:\", ridge_grid.best_params_)\n",
        "\n",
        "# Grid search for Lasso\n",
        "lasso_grid = GridSearchCV(Lasso(), param_grid, cv=5, scoring=\"r2\")\n",
        "lasso_grid.fit(X_train, y_train)\n",
        "print(\"Best Alpha for Lasso:\", lasso_grid.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WhFGt5-U2tY",
        "outputId": "f4e81389-76d0-40fd-9dc8-cafba8afa0bf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Alpha for Ridge: {'alpha': 1}\n",
            "Best Alpha for Lasso: {'alpha': 0.001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå 6. Can Ridge & Lasso be combined?**\n",
        "\n",
        "Yes! **Elastic Net** combines both:\n",
        "\n",
        "$\n",
        "Loss = MSE + \\lambda_1 \\sum |w| + \\lambda_2 \\sum w^2\n",
        "$"
      ],
      "metadata": {
        "id": "ddJPYFm5UOCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Mix of L1 and L2\n",
        "elastic_net.fit(X_train, y_train)\n",
        "print(\"Elastic Net Coefficients:\\n\", elastic_net.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ865mPfUY_q",
        "outputId": "fd6323ed-020d-4f32-a2b5-fb413bfc8303"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Coefficients:\n",
            " [ 4.45623881e-05 -8.11422095e-02 -0.00000000e+00  1.29814793e-02\n",
            " -0.00000000e+00 -2.52964788e-01  0.00000000e+00 -0.00000000e+00\n",
            "  0.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå 7. What Are the Assumptions of Linear Regression?**\n",
        "\n",
        "Linear Regression works best when the data satisfies certain assumptions. Violating these assumptions can lead to biased coefficients, incorrect inferences, and poor predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**‚úÖ 1. Linearity Assumption**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "The relationship between the independent variables (`X`) and the dependent variable (`y`) must be linear.\n",
        "\n",
        "$\n",
        "y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\epsilon\n",
        "$\n",
        "\n",
        "**Why is this important?**\n",
        "- If the relationship is non-linear, Linear Regression will fail to capture the patterns in the data.\n",
        "\n",
        "**How to check for linearity?**\n",
        "\n",
        "1Ô∏è‚É£ Scatter Plot of Features vs. Target (`y`)  \n",
        "- Plot each independent variable (`X`) against `y` and look for linear trends.\n",
        "- If the relationships are curved, the assumption is violated.\n",
        "\n",
        "2Ô∏è‚É£ Residuals vs. Fitted Values Plot\n",
        "- Residuals should be randomly scattered around zero.\n",
        "- If residuals show a pattern (e.g., curve or wave), the relationship may be non-linear.\n",
        "\n",
        "**How to fix non-linearity?**\n",
        "‚úÖ Use Polynomial Regression (add \\(X^2, X^3\\) terms).  \n",
        "‚úÖ Use log transformations on the dependent variable (`y`).\n",
        "\n",
        "---\n",
        "\n",
        "**‚úÖ 2. No Multicollinearity Assumption**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "Independent variables (`X1, X2, X3, ...`) should not be highly correlated with each other.\n",
        "\n",
        "**Why is this important?**\n",
        "- When features are highly correlated, the model cannot determine which feature has more influence, leading to unstable coefficients.\n",
        "- It causes large standard errors, making p-values unreliable.\n",
        "\n",
        "**How to detect multicollinearity?**\n",
        "\n",
        "1Ô∏è‚É£ Correlation Matrix (Heatmap)\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
        "plt.show()\n",
        "```\n",
        "- Look for high correlations (‚â• 0.8).\n",
        "\n",
        "2Ô∏è‚É£ Variance Inflation Factor (VIF)\n",
        "```python\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)\n",
        "```\n",
        "- VIF > 5 or 10 ‚Üí Multicollinearity issue.\n",
        "\n",
        "**How to fix multicollinearity?**\n",
        "\n",
        "‚úÖ Remove one of the highly correlated features.  \n",
        "‚úÖ Use Principal Component Analysis (PCA) to reduce dimensionality.  \n",
        "‚úÖ Use Ridge Regression (L2 Regularization) to shrink correlated feature weights.\n",
        "\n",
        "---\n",
        "\n",
        "**‚úÖ 3. Homoscedasticity (Constant Variance of Residuals)**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "The spread (variance) of residuals should be constant across all values of `X`.\n",
        "\n",
        "**Why is this important?**\n",
        "- If variance is not constant (heteroscedasticity), predictions become less reliable for certain values of `X`.\n",
        "- Leads to biased standard errors ‚Üí affects hypothesis testing.\n",
        "\n",
        "**How to check for homoscedasticity?**\n",
        "\n",
        "1Ô∏è‚É£ Residuals vs. Fitted Values Plot\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "residuals = y_test - model.predict(X_test)\n",
        "\n",
        "plt.scatter(model.predict(X_test), residuals)\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs. Predicted\")\n",
        "plt.show()\n",
        "```\n",
        "- If points are randomly scattered ‚Üí Assumption holds.\n",
        "- If points fan out (cone shape) ‚Üí Heteroscedasticity detected.\n",
        "\n",
        "**How to fix heteroscedasticity?**\n",
        "‚úÖ Apply log transformation on `y` (`log(y)`).  \n",
        "‚úÖ Use Weighted Least Squares Regression (WLS).  \n",
        "‚úÖ Add missing independent variables that could explain variance.\n",
        "\n",
        "---\n",
        "\n",
        "**‚úÖ 4. Normality of Residuals**\n",
        "**Definition:**\n",
        "\n",
        "Residuals should be normally distributed.\n",
        "\n",
        "**Why is this important?**\n",
        "- Normality ensures that confidence intervals and hypothesis tests remain valid.\n",
        "- If residuals are not normal, predictions and inferences can be misleading.\n",
        "\n",
        "**How to check for normality?**\n",
        "\n",
        "1Ô∏è‚É£ Histogram of Residuals\n",
        "```python\n",
        "import seaborn as sns\n",
        "sns.histplot(residuals, kde=True)\n",
        "```\n",
        "- Should look like a normal (bell-shaped) curve.\n",
        "\n",
        "2Ô∏è‚É£ Q-Q Plot (Quantile-Quantile Plot)\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plt.show()\n",
        "```\n",
        "- Points should lie on a straight diagonal line.\n",
        "- Curved pattern? ‚Üí Residuals are not normal.\n",
        "\n",
        "**How to fix non-normal residuals?**\n",
        "‚úÖ Use log transformation on `y`.  \n",
        "‚úÖ Add missing variables to the model.  \n",
        "‚úÖ Use robust regression (Huber loss instead of MSE).\n",
        "\n",
        "---\n",
        "\n",
        "**üî• Summary Table: Assumptions & Fixes**\n",
        "\n",
        "| **Assumption** | **How to Detect** | **How to Fix** |\n",
        "|--------------|------------------|----------------|\n",
        "| **Linearity** | Scatter plots, Residuals vs. Fitted | Use Polynomial Regression, Log Transformation |\n",
        "| **No Multicollinearity** | Correlation Matrix, VIF > 5 | Remove correlated features, Use Ridge Regression |\n",
        "| **Homoscedasticity** | Residuals vs. Fitted Plot | Log Transformation, Weighted Least Squares |\n",
        "| **Normality of Residuals** | Histogram, Q-Q Plot | Log Transformation, Use Robust Regression |\n",
        "\n",
        "---\n",
        "üéØ **Can You Answer These?**\n",
        "\n",
        "1Ô∏è‚É£ **What is the effect of multicollinearity on feature importance?**  \n",
        "2Ô∏è‚É£ **How does heteroscedasticity impact confidence intervals?**  \n",
        "3Ô∏è‚É£ **Why does Ridge Regression help with multicollinearity but not Lasso?**  "
      ],
      "metadata": {
        "id": "0C2zz-S1X2gF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìå Interview Questions & Answers**\n",
        "1. **What are the assumptions of Linear Regression?**  \n",
        "   - Linearity  \n",
        "   - No multicollinearity  \n",
        "   - Homoscedasticity (constant variance)  \n",
        "   - Normality of residuals  \n",
        "\n",
        "2. **How to handle multicollinearity in regression?**  \n",
        "   - Use Ridge Regression (L2 Regularization)  \n",
        "   - Use Feature Selection (VIF, PCA)  \n",
        "\n",
        "3. **How to interpret Logistic Regression coefficients?**  \n",
        "   - A positive coefficient means the feature increases probability of positive class.  \n",
        "   - A negative coefficient means the feature decreases probability of positive class.  \n",
        "\n",
        "4. **What is the difference between MSE and Log Loss?**  \n",
        "   - MSE (Mean Squared Error) ‚Üí Used in Linear Regression (continuous output).  \n",
        "   - Log Loss (Binary Cross-Entropy) ‚Üí Used in Logistic Regression (classification).\n",
        "\n",
        "5. **Why does Linear Regression use MSE, but Logistic Regression uses Log Loss?**  \n",
        "   - MSE assumes errors are normally distributed ‚Üí used for continuous outputs.  \n",
        "   - Log Loss measures classification probability errors ‚Üí used for binary outputs.\n",
        "\n",
        "6. **What happens if independent variables in Linear Regression are highly correlated?**  \n",
        "   - Leads to Multicollinearity ‚Üí Fix it by using Ridge Regression (L2 regularization).\n",
        "\n",
        "7. **How do you interpret Logistic Regression coefficients?**  \n",
        "   - Convert them to odds ratio:\n",
        "\n",
        "     $ e^{coefficient} $ $= change in odds for 1 unit increase in feature$\n",
        "\n",
        "8. **How to handle class imbalance in Logistic Regression?**  \n",
        "   - Use `class_weight=\"balanced\"` in `LogisticRegression()`.  \n",
        "   - Try Resampling (SMOTE) to balance classes.\n",
        "\n",
        "9. **When to use Ridge vs. Lasso?**\n",
        "- Use Ridge when you want to keep all features but reduce their impact.\n",
        "- Use Lasso when you want to perform feature selection.\n",
        "\n",
        "10. **What happens when $ \\lambda $ is too large?**\n",
        "- Ridge: Shrinks all weights close to zero but keeps them.\n",
        "- Lasso: Shrinks most weights to zero, removing features.\n",
        "\n",
        "11. **How does Lasso perform feature selection?**\n",
        "- The L1 penalty forces some weights to exactly zero.\n",
        "- This means the corresponding features are dropped.\n",
        "\n",
        "12. **What happens if the error terms are not normally distributed in Linear Regression?**\n",
        "- If the errors (residuals) are not normally distributed, the confidence intervals and hypothesis tests become unreliable.\n",
        "- If sample size is large, the Central Limit Theorem (CLT) helps mitigate this issue.\n",
        "- If normality is a major concern, consider log transformation of the dependent variable.\n",
        "\n",
        "13. **What is Homoscedasticity? Why is it important?**\n",
        "- Homoscedasticity means that the variance of residuals (errors) is constant across all values of independent variables.\n",
        "- Violation of homoscedasticity ‚Üí heteroscedasticity (changing variance) ‚Üí Leads to biased standard errors, affecting hypothesis tests.\n",
        "\n",
        "üí° Fix:  \n",
        "- Use Weighted Least Squares Regression (WLS).\n",
        "- Transform data (e.g., log transformation of target variable).\n",
        "\n",
        "14. **What happens if you have outliers in Linear Regression?**\n",
        "- Outliers can heavily influence the regression model, making coefficients unstable.\n",
        "- Outliers increase variance and decrease model robustness.\n",
        "\n",
        "üí° Fix:  \n",
        "- Use Robust Regression (e.g., **Huber Loss** instead of MSE).  \n",
        "- Remove extreme outliers using IQR method.  \n",
        "\n",
        "15. **What is the difference between R¬≤ and Adjusted R¬≤?**\n",
        "\n",
        "| Metric | Definition | When to Use |\n",
        "|--------|------------|------------|\n",
        "| **R¬≤ (Coefficient of Determination)** | Measures how much variance in `y` is explained by `X` | Basic performance metric |\n",
        "| **Adjusted R¬≤** | Penalizes adding too many unnecessary features | When comparing models with different numbers of features |\n",
        "\n",
        "**Formula for Adjusted R¬≤:**\n",
        "$\n",
        "Adjusted R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n",
        "$\n",
        "Where:\n",
        "- $ n $ = number of observations\n",
        "- $ k $ = number of features\n",
        "\n",
        "üí° Key Takeaway:  \n",
        "- **Adjusted R¬≤ is always ‚â§ R¬≤.**\n",
        "- If adding more features increases Adjusted R¬≤, they are useful.\n",
        "\n",
        "16. **Can Linear Regression work for non-linear relationships?**\n",
        "- No, because Linear Regression assumes a linear relationship between X and y.\n",
        "- If the relationship is non-linear, you can:\n",
        "  - Use Polynomial Regression (add non-linear terms $X^2, X^3$).\n",
        "  - Use Decision Trees / Neural Networks** for complex relationships.\n",
        "\n",
        "17. **Why do we need the Sigmoid function in Logistic Regression?**\n",
        "- The Sigmoid function transforms any input into a probability (0 to 1).\n",
        "- Without the sigmoid, Logistic Regression would behave like Linear Regression.\n",
        "\n",
        "$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$\n",
        "\n",
        "üí° **Key Takeaway:**  \n",
        "- The Sigmoid output represents the probability of class 1.\n",
        "- If $ \\sigma(z) > 0.5 $ ‚Üí Predict class 1, else class 0.\n",
        "\n",
        "18. **What is the decision boundary in Logistic Regression?**\n",
        "- The decision boundary separates the two classes.\n",
        "- It is a hyperplane in high-dimensional space.\n",
        "- Logistic Regression assumes a linear decision boundary.\n",
        "\n",
        "üí° **Non-linear decision boundary?**  \n",
        "- Use Polynomial Features or Kernel methods (SVM).\n",
        "\n",
        "19. **How does regularization help in Logistic Regression?**\n",
        "- Prevents overfitting by penalizing large coefficients.\n",
        "- L1 (Lasso) Regularization ‚Üí Shrinks some coefficients to exactly zero (feature selection).\n",
        "- L2 (Ridge) Regularization ‚Üí Shrinks coefficients but keeps all features.\n",
        "\n",
        "20. **How do you handle multicollinearity in Logistic Regression?**\n",
        "- Multicollinearity occurs when independent variables are highly correlated.\n",
        "- It leads to unstable coefficients and high variance.\n",
        "\n",
        "üí° **Fix:**\n",
        "- Use Ridge Regression (L2 Regularization).\n",
        "- Remove highly correlated features using VIF (Variance Inflation Factor).\n",
        "\n",
        "21. **Why can't Logistic Regression be solved using the Normal Equation?**\n",
        "- The Normal Equation works for Linear Regression because MSE is convex.\n",
        "- Logistic Regression uses Log Loss, which is not convex and has no closed-form solution.\n",
        "- Instead, we use Gradient Descent to optimize weights.\n",
        "\n",
        "22. **What is the main difference between Ridge and Lasso Regression?**\n",
        "\n",
        "| **Regularization Type** | **Formula** | **Effect on Coefficients** | **Best Used When** |\n",
        "|-----------------|-----------------|------------------------|---------------------|\n",
        "| **Ridge Regression (L2)** | $ \\lambda \\sum w^2 $ | Shrinks all coefficients | When **all features contribute** a little |\n",
        "| **Lasso Regression (L1)** | $ \\lambda \\sum |w| $ | Forces some coefficients to **zero** | When **feature selection is needed** |\n",
        "\n",
        "23. **What happens when $ \\lambda $ is too large in Ridge and Lasso?**\n",
        "- Ridge Regression: Shrinks all weights towards zero, but keeps all features.\n",
        "- Lasso Regression: Shrinks some weights to zero, removing features.\n",
        "\n",
        "üí° Fix:  \n",
        "- Tune $ \\lambda $ using `GridSearchCV`.\n",
        "\n",
        "24. **What is Elastic Net?**\n",
        "Elastic Net combines Ridge (L2) and Lasso (L1) penalties:\n",
        "\n",
        "$\n",
        "Loss = MSE + \\lambda_1 \\sum |w| + \\lambda_2 \\sum w^2\n",
        "$\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "elastic_net.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "üí° Use Elastic Net when:  \n",
        "- Some features should be removed but others should be regularized.\n",
        "\n",
        "25. **Why does Lasso Regression perform feature selection?**\n",
        "- L1 regularization forces some weights to exactly 0.\n",
        "- Features with zero coefficients are effectively removed from the model.\n",
        "\n",
        "üí° Lasso is useful when the dataset has many irrelevant features.\n",
        "\n",
        "**üî• Bonus Brain Teasers**\n",
        "\n",
        "**26. What if you standardize data before applying Ridge & Lasso?**\n",
        "- Standardization improves regularization.\n",
        "- Ridge/Lasso work better when all features are on the same scale.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "```\n",
        "\n",
        "27. **What is the effect of feature scaling on Linear & Logistic Regression?**\n",
        "\n",
        "| **Without Scaling** | **With Scaling** |\n",
        "|------------------|------------------|\n",
        "| Model converges slowly | Model converges faster |\n",
        "| Ridge/Lasso penalizes large-scale features unfairly | Regularization applies correctly |\n",
        "| Gradient Descent oscillates | More stable updates |\n",
        "\n",
        "üí° Always scale features when using Ridge, Lasso, or Logistic Regression!\n",
        "\n",
        "\n",
        "28. **Why does Ridge Regression never set coefficients to zero?**\n",
        "- Ridge Regression (L2 Regularization) adds a squared penalty to the coefficients:\n",
        "  \n",
        "$\n",
        "  Loss = MSE + \\lambda \\sum w_i^2\n",
        "$\n",
        "\n",
        "- The squared penalty shrinks the coefficients towards zero but never completely eliminates them.\n",
        "- Since the penalty is proportional to $ w^2 $, it reduces the magnitude of the coefficients but does not make any of them exactly zero.\n",
        "- Ridge keeps all features but assigns smaller weights, making it different from Lasso Regression, which can force coefficients to be exactly zero due to its L1 penalty.\n",
        "\n",
        "29. **How does Logistic Regression handle non-linearity?**\n",
        "Logistic Regression assumes a linear decision boundary, but we can handle non-linearity using:\n",
        "\n",
        "1. Polynomial Features\n",
        "- Transform features to capture non-linear relationships:\n",
        "  \n",
        "  ```python\n",
        "  from sklearn.preprocessing import PolynomialFeatures\n",
        "  \n",
        "  poly = PolynomialFeatures(degree=2)\n",
        "  X_poly = poly.fit_transform(X)\n",
        "  ```\n",
        "- This allows the model to fit curved decision boundaries.\n",
        "\n",
        "2. Feature Engineering\n",
        "- Transformations like log, exponential, and interactions can help model non-linearity.\n",
        "\n",
        "\n",
        "30. **Why does increasing $ \\lambda $ make models simpler?**\n",
        "- $ \\lambda $ controls the strength of regularization in Ridge and Lasso Regression:\n",
        "\n",
        "  $\n",
        "  Loss = MSE + \\lambda \\sum |w| \\quad \\text{(Lasso)}\n",
        "  $\n",
        "  $\n",
        "  Loss = MSE + \\lambda \\sum w^2 \\quad \\text{(Ridge)}\n",
        "  $\n",
        "\n",
        "- When $ \\lambda $ is small, regularization is weak, and the model learns all patterns (including noise).\n",
        "- When $ \\lambda $ is large, it shrinks coefficients more aggressively, reducing model complexity and making it more generalizable.\n",
        "- High $ \\lambda $ ‚Üí Simpler model with fewer features contributing significantly.\n",
        "\n",
        "üí° **Key Takeaway:**  \n",
        "Increasing $ \\lambda $ reduces variance but increases bias, enforcing simplicity and preventing overfitting.\n"
      ],
      "metadata": {
        "id": "k70wD3IKS-yl"
      }
    }
  ]
}
