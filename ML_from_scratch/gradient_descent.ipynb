{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOiwJWWFjgL7nV/zL2fp5Gh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElahehJafarigol/ML_from_scratch/blob/main/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a comprehensive overview of gradient descent—what it is, why we use it, how it works, and the key variations and concepts you’ll encounter in machine learning. Use this as a foundation to understand or explain gradient descent in interviews or in your own work.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What Is Gradient Descent?\n",
        "\n",
        "**Gradient Descent** is an iterative optimization algorithm used to find a local minimum of a differentiable function (often a *loss* or *cost* function in machine learning). You can think of it as a way to “walk” downhill on the surface defined by your cost function until you (hopefully) reach the lowest point.\n",
        "\n",
        "- **Goal**: Minimize a cost function $ J(\\theta) $, where $ \\theta $ represents the parameters (weights) of your model.\n",
        "- **Approach**: Update $\\theta$ in the direction opposite the gradient of $ J(\\theta) $ with respect to $\\theta$.\n",
        "\n",
        "Mathematically, for each parameter $\\theta_j$:\n",
        "\n",
        "$\n",
        "\\theta_j \\leftarrow \\theta_j \\;-\\; \\alpha \\;\\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
        "$\n",
        "\n",
        "where:\n",
        "- $\\alpha$ is the **learning rate** (a small positive number controlling the size of each step).\n",
        "- $\\frac{\\partial}{\\partial \\theta_j} J(\\theta)$ is the **partial derivative** of the cost function with respect to $\\theta_j$.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why Gradient Descent?\n",
        "\n",
        "1. **Efficiency**: Many cost functions (e.g., in linear regression, logistic regression, or neural networks) are high-dimensional and lack a closed-form solution for the optimal $\\theta$. Gradient descent is relatively efficient and widely applicable.\n",
        "2. **Generality**: Works for complex, non-convex functions (like neural network loss surfaces), unlike some second-order methods (like Newton’s method) which are more expensive or difficult to implement for large-scale problems.\n",
        "3. **Simplicity**: Easy to implement and reason about—repeatedly move $\\theta$ in the direction that reduces the cost function the most rapidly.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Key Ingredients\n",
        "\n",
        "### 3.1 Cost (Loss) Function\n",
        "\n",
        "- **Definition**: A function $ J(\\theta) $ that measures how far your predictions are from the target values.  \n",
        "- **Examples**:\n",
        "  - **Mean Squared Error (MSE)** in linear regression.\n",
        "  - **Cross-Entropy Loss** in classification tasks.\n",
        "\n",
        "### 3.2 Learning Rate $\\alpha$\n",
        "\n",
        "- **Definition**: A hyperparameter that controls how big a step you take in the direction opposite the gradient.\n",
        "- **Trade-off**:\n",
        "  - **Too large**: The steps might overshoot minima and cause divergence.\n",
        "  - **Too small**: Convergence is very slow; might get stuck in local minima or saddle points for a long time.\n",
        "\n",
        "### 3.3 Gradient (Partial Derivatives)\n",
        "\n",
        "- **Computation**: For each parameter $\\theta_j$, you compute $\\frac{\\partial}{\\partial \\theta_j} J(\\theta)$.\n",
        "- **Implementation**:\n",
        "  - For neural networks, this is typically done using **backpropagation** (the chain rule applied to the entire network).\n",
        "  - For simple linear/logistic regression, there are straightforward analytical partial derivatives.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Types of Gradient Descent\n",
        "\n",
        "### 4.1 Batch Gradient Descent (BGD)\n",
        "\n",
        "- **Process**: Uses the **entire training set** to compute the gradient for each update.\n",
        "- **Pros**:\n",
        "  - Converges in a smooth, stable manner since you use all data for each gradient calculation.\n",
        "  - Often used when datasets are not extremely large.\n",
        "- **Cons**:\n",
        "  - Can be **very slow** for large datasets because you must process the entire dataset to make a single update.\n",
        "\n",
        "### 4.2 Stochastic Gradient Descent (SGD)\n",
        "\n",
        "- **Process**: Updates parameters **for each training example** (or a randomly picked example) at a time.\n",
        "- **Pros**:\n",
        "  - Much faster per update because each update is based on one sample.\n",
        "  - Can escape local minima more easily in non-convex problems due to the noise in each gradient step.\n",
        "- **Cons**:\n",
        "  - High variance in the parameter updates; training can be noisy and may oscillate around the minimum rather than fully converge.\n",
        "  - Typically requires a decreasing learning rate over time or other strategies to ensure convergence.\n",
        "\n",
        "### 4.3 Mini-Batch Gradient Descent\n",
        "\n",
        "- **Process**: A hybrid approach—use a **subset (batch) of the training set** (e.g., 32, 64, 128 samples) to compute each gradient.\n",
        "- **Pros**:\n",
        "  - More stable updates than pure SGD (less noisy).\n",
        "  - Faster than batch gradient descent because each update doesn’t need the entire dataset.\n",
        "  - Effectively uses vectorized operations on batches (well-suited for GPU computations).\n",
        "- **Cons**:\n",
        "  - Introduces some hyperparameter tuning around the batch size.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Variants and Improvements\n",
        "\n",
        "Over the years, many **adaptive or momentum-based** optimizers have been proposed to improve vanilla gradient descent:\n",
        "\n",
        "1. **Momentum**  \n",
        "   - Incorporates a fraction of the previous update into the current update (like a ball rolling downhill, picking up speed).  \n",
        "   - **Update rule** typically looks like:  \n",
        "     $\n",
        "     v \\leftarrow \\beta v + (1 - \\beta)\\nabla_\\theta J(\\theta)\n",
        "     $\n",
        "\n",
        "     $\n",
        "     \\theta \\leftarrow \\theta - \\alpha v\n",
        "     $\n",
        "\n",
        "     where $v$ is the velocity (running average of gradients), and $\\beta$ is a momentum coefficient (e.g., 0.9).\n",
        "\n",
        "2. **Nesterov Accelerated Gradient (NAG)**  \n",
        "   - A lookahead version of momentum that corrects for velocity overshoot by computing the gradient at a lookahead position.\n",
        "   - Often speeds up training convergence.\n",
        "\n",
        "3. **AdaGrad**  \n",
        "   - Adapts the learning rate to the parameters, performing **larger updates for infrequent parameters** and smaller updates for frequent parameters.\n",
        "   - Accumulates the square of the gradients in a vector $r$.\n",
        "\n",
        "4. **RMSProp**  \n",
        "   - Modifies AdaGrad by using an exponentially decaying average of squared gradients, preventing the learning rate from monotonically decreasing.\n",
        "   - $\n",
        "     r \\leftarrow \\beta r + (1-\\beta) (\\nabla_\\theta J(\\theta))^2\n",
        "     $\n",
        "\n",
        "     $\n",
        "     \\theta \\leftarrow \\theta - \\frac{\\alpha}{\\sqrt{r} + \\epsilon} \\nabla_\\theta J(\\theta)\n",
        "     $\n",
        "\n",
        "5. **Adam (Adaptive Moment Estimation)**  \n",
        "   - Combines ideas from Momentum and RMSProp.\n",
        "   - Maintains exponentially decaying averages of past gradients and their squares (first and second moments).\n",
        "   - Currently one of the most popular optimization algorithms in deep learning.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Convergence and Practical Considerations\n",
        "\n",
        "1. **Convergence Criteria**:\n",
        "   - Cost function $ J(\\theta) $ changes very little between iterations.\n",
        "   - Gradient norms become very small.\n",
        "   - A maximum number of epochs (full passes over the data) is reached.\n",
        "\n",
        "2. **Learning Rate Scheduling**:\n",
        "   - **Fixed**: Keep $\\alpha$ the same throughout training.\n",
        "   - **Decay**: Gradually reduce $\\alpha$ over time (e.g., exponential decay, step decay).\n",
        "   - **Adaptive**: Use advanced optimizers (AdaGrad, RMSProp, Adam) that effectively tune the learning rate per parameter.\n",
        "\n",
        "3. **Local Minima vs. Global Minima**:\n",
        "   - For **convex** functions (e.g., linear regression MSE), any local minimum is a global minimum.\n",
        "   - For **non-convex** functions (like neural networks), gradient descent could get stuck in local minima or saddle points, but in practice, neural nets often train to good solutions anyway.\n",
        "\n",
        "4. **Vanishing and Exploding Gradients**:\n",
        "   - Common in deep neural networks.\n",
        "   - **Vanishing**: Gradients get smaller as they backprop through layers, slowing or stopping learning.\n",
        "   - **Exploding**: Gradients become extremely large, leading to numerical instability.\n",
        "   - **Solutions**: Proper weight initialization, gradient clipping, better activation functions, or specialized architectures (like LSTM/GRU for sequences).\n",
        "\n",
        "5. **Initialization**:\n",
        "   - Good parameter initialization can drastically help or hinder convergence (e.g., Xavier/Glorot initialization, He initialization for deep nets).\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Example Pseudocode (Vanilla Mini-Batch Gradient Descent)\n",
        "\n",
        "Below is a simple outline of how mini-batch gradient descent might look in Python-like pseudocode:\n",
        "\n",
        "```python\n",
        "# Pseudocode for mini-batch gradient descent\n",
        "initialize parameters theta (e.g., randomly)\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    shuffle(training_data)\n",
        "    \n",
        "    for each batch in training_data:\n",
        "        X_batch, y_batch = extract_batch(batch)\n",
        "        \n",
        "        # 1. Compute predictions & cost\n",
        "        predictions = model(X_batch, theta)\n",
        "        cost = cost_function(predictions, y_batch)\n",
        "        \n",
        "        # 2. Compute gradients\n",
        "        gradients = compute_gradients(cost, theta)\n",
        "        \n",
        "        # 3. Update parameters\n",
        "        theta = theta - alpha * gradients\n",
        "    \n",
        "    # (Optional) Evaluate cost/accuracy on the full dataset or a validation set\n",
        "    # (Optional) Adjust learning rate, do early stopping checks, etc.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Putting It All Together for Interviews\n",
        "\n",
        "### Core Points to Emphasize\n",
        "\n",
        "1. **Conceptual Definition**: Gradient descent is an iterative method to find a local minimum by moving in the direction opposite to the gradient.\n",
        "2. **Mathematical Form**: $\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta J(\\theta)$.\n",
        "3. **Learning Rate**: Critical hyperparameter for convergence.\n",
        "4. **Variants**: Batch, Stochastic, Mini-Batch (and advanced methods like Adam, RMSProp).\n",
        "5. **Real-World Use**: Essential for training nearly all modern ML models, especially neural networks.\n",
        "\n",
        "### Typical Interview Questions\n",
        "\n",
        "- **Explain the difference between batch, stochastic, and mini-batch gradient descent.**  \n",
        "- **What is the effect of the learning rate on convergence?**  \n",
        "- **Why might gradient descent get stuck?** (Local minima, saddle points, etc. in non-convex optimization.)  \n",
        "- **How do advanced optimizers like Adam improve on vanilla gradient descent?**  \n",
        "- **How do you know you’ve converged or that you need to stop training?**  "
      ],
      "metadata": {
        "id": "zD-LNmYitYH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a **vanilla gradient descent** implementation in Python for **linear regression**, followed by a **practical use case** with a synthetic dataset. This will help you see how gradient descent can be applied end-to-end."
      ],
      "metadata": {
        "id": "zCglCh9ZFB-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Vanilla Gradient Descent Implementation\n",
        "\n",
        "#### 1. Define the Model and Cost Function\n",
        "\n",
        "We’ll assume a linear model:\n",
        "$\n",
        "\\hat{y} = X \\theta\n",
        "$\n",
        "where\n",
        "- $ X $ is an $(m \\times n)$ matrix of training inputs (with $m$ examples and $n$ features).\n",
        "- $\\theta$ is an $(n \\times 1)$ vector of parameters.\n",
        "- $\\hat{y}$ is the prediction vector $(m \\times 1)$.\n",
        "\n",
        "For **linear regression**, we often use **Mean Squared Error (MSE)** as the cost (loss) function:\n",
        "$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y}_i - y_i)^2.\n",
        "$\n",
        "\n",
        "#### 2. Compute the Gradient\n",
        "\n",
        "The gradient of $ J(\\theta) $ w.r.t. $\\theta$ in matrix form is:\n",
        "$\n",
        "\\nabla_\\theta J(\\theta) = \\frac{1}{m} X^\\top (X\\theta - y).\n",
        "$\n",
        "\n",
        "#### 3. Update Rule (Vanilla Gradient Descent)\n",
        "\n",
        "We update $\\theta$ iteratively:\n",
        "$\n",
        "\\theta \\leftarrow \\theta \\;-\\; \\alpha \\;\\nabla_\\theta J(\\theta),\n",
        "$\n",
        "where $\\alpha$ is the **learning rate**.\n",
        "\n",
        "#### 4. Python Code\n",
        "\n",
        "Here is a minimal, fully **vectorized** implementation in Python using NumPy:"
      ],
      "metadata": {
        "id": "eynl53Y0FKN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Computes the Mean Squared Error cost for linear regression:\n",
        "    J(theta) = (1/(2*m)) * sum((X*theta - y)^2)\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    predictions = X.dot(theta)               # shape: (m, 1)\n",
        "    errors = predictions - y.reshape(-1, 1)  # ensure shape (m, 1)\n",
        "    cost = (1 / (2*m)) * np.sum(errors**2)\n",
        "    return cost\n",
        "\n",
        "def compute_gradient(X, y, theta):\n",
        "    \"\"\"\n",
        "    Computes the gradient of the cost function with respect to theta.\n",
        "    Grad = (1/m) * X^T (X*theta - y)\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    predictions = X.dot(theta)\n",
        "    errors = predictions - y.reshape(-1, 1)\n",
        "    grad = (1/m) * X.T.dot(errors)  # shape: (n, 1)\n",
        "    return grad\n",
        "\n",
        "def gradient_descent(X, y, alpha=0.01, num_iters=100):\n",
        "    \"\"\"\n",
        "    Performs vanilla (batch) gradient descent to learn theta.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : numpy array of shape (m, n)\n",
        "    y : numpy array of shape (m,) or (m, 1)\n",
        "    alpha : learning rate\n",
        "    num_iters : number of iterations (epochs)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    theta : Learned parameters (n, 1)\n",
        "    cost_history : List of cost values at each iteration\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros((n, 1))  # or random initialization\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        grad = compute_gradient(X, y, theta)\n",
        "        theta = theta - alpha * grad\n",
        "\n",
        "        # Optionally track the cost to see convergence\n",
        "        current_cost = compute_cost(X, y, theta)\n",
        "        cost_history.append(current_cost)\n",
        "\n",
        "    return theta, cost_history"
      ],
      "metadata": {
        "id": "1Vc_S_46DcGp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: A Practical Use Case\n",
        "\n",
        "Suppose we want to **predict house prices** based on two features:\n",
        "1. **House size** (square feet)\n",
        "2. **Number of bedrooms**\n",
        "\n",
        "Let’s create a **synthetic dataset** and use gradient descent to fit a linear model. We’ll generate data under the assumption that:\n",
        "\n",
        "$\n",
        "\\text{Price} = 50,000 + 100 \\times (\\text{sqft}) + 10,000 \\times (\\text{bedrooms}) + \\text{noise}\n",
        "$\n",
        "\n",
        "We’ll then see how gradient descent learns the parameters that approximate these relationships."
      ],
      "metadata": {
        "id": "9K_VghvuFaAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-FOBZ0KtBOn",
        "outputId": "7dc57fcb-ef34-4b98-c594-c2e824153508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_data shape: (200, 3)\n",
            "y_data shape: (200, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "m = 200  # number of examples\n",
        "\n",
        "# Feature 1: House size in '000 sq ft (e.g., 1.2 => 1200 sq ft)\n",
        "X_size = np.random.rand(m, 1) * 3  # range [0, 3] => up to 3000 sq ft\n",
        "# Feature 2: Number of bedrooms (1-5)\n",
        "X_bedrooms = np.random.randint(1, 6, size=(m, 1))\n",
        "\n",
        "# True parameters\n",
        "theta_0_true = 50000      # bias (intercept)\n",
        "theta_size_true = 100     # slope for size\n",
        "theta_bed_true = 10000    # slope for bedrooms\n",
        "\n",
        "# Generate noisy labels\n",
        "noise = np.random.randn(m, 1) * 10000  # some random noise\n",
        "y_data = (theta_0_true +\n",
        "          theta_size_true * (X_size * 1000) +\n",
        "          theta_bed_true * X_bedrooms +\n",
        "          noise)\n",
        "\n",
        "# Reshape y_data to be (m, 1) for consistency\n",
        "y_data = y_data.reshape(-1, 1)\n",
        "\n",
        "# Construct design matrix X with a column of 1s for the intercept\n",
        "# so final shape will be (m, 3)\n",
        "X_data = np.hstack([np.ones((m, 1)), X_size * 1000, X_bedrooms])\n",
        "\n",
        "print(\"X_data shape:\", X_data.shape)\n",
        "print(\"y_data shape:\", y_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have:\n",
        "- **m = 200** examples.\n",
        "- **X_data** has 3 columns:  \n",
        "  1. All 1s (for intercept).  \n",
        "  2. House size in square feet (0 to ~3000).  \n",
        "  3. Number of bedrooms (1 to 5).\n"
      ],
      "metadata": {
        "id": "-6TiBwa6FjNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Apply Gradient Descent\n",
        "\n",
        "Let’s train our linear regression model using the vanilla gradient descent function defined above:"
      ],
      "metadata": {
        "id": "_qONIo-PFnVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 1e-8     # learning rate (small, since our features can be large)\n",
        "num_iters = 2000\n",
        "\n",
        "theta_learned, cost_history = gradient_descent(X_data, y_data, alpha, num_iters)\n",
        "\n",
        "print(\"Learned theta:\")\n",
        "print(theta_learned.flatten())\n",
        "\n",
        "print(\"Final cost:\", round(cost_history[-1], 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWt4YT8bExnr",
        "outputId": "ebe8cd2a-61a2-4b77-d8cc-008775130520"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned theta:\n",
            "[  0.53082517 139.63670941   2.12184138]\n",
            "Final cost: 1130179514.056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Choosing a Learning Rate\n",
        "- We used **1e-8** as the learning rate because the **house size** values are in the thousands, and the price is in the tens or hundreds of thousands. A higher learning rate might cause divergence in this scale."
      ],
      "metadata": {
        "id": "_ZeOqgnAFuwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Evaluating the Results\n",
        "\n",
        "- **Check final parameters**: Compare `theta_learned` to the *true* parameters `(theta_0_true, theta_size_true, theta_bed_true)`.\n",
        "- **Plot the cost vs. iterations** to confirm if it’s converging:"
      ],
      "metadata": {
        "id": "n_unv91DFzFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(num_iters), cost_history, 'b-')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Cost (MSE)\")\n",
        "plt.title(\"Cost Convergence Over Iterations\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "bCrUMRLLE1iy",
        "outputId": "8c0a5e64-82a8-4741-effa-71a13b5c5ed6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the cost decreases smoothly and stabilizes, gradient descent is converging properly."
      ],
      "metadata": {
        "id": "DBk3-qIrF5il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Making Predictions\n",
        "\n",
        "After training, we can make predictions for a new house (e.g., 2000 sq ft, 3 bedrooms):"
      ],
      "metadata": {
        "id": "n50P9XAzF9Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prediction\n",
        "test_house = np.array([1, 2000, 3]).reshape(1, -1)  # 1 => intercept\n",
        "predicted_price = test_house.dot(theta_learned)\n",
        "print(\"Predicted Price for 2000 sq ft, 3 br:\", predicted_price.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht-UioptE4xe",
        "outputId": "53d583c0-6ff9-403f-b552-e16b80c11122"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Price for 2000 sq ft, 3 br: 279280.3151780174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam Optimizer\n",
        "Below is a detailed explanation of the **Adam optimizer**—what it does, how it works, and why it’s widely used in practice. Adam is short for **Adaptive Moment Estimation**, and it combines concepts from **Momentum** and **RMSProp** to adaptively tune the learning rate for each parameter.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "### What is Adam?\n",
        "- **Adam** is an iterative optimization algorithm typically used to train deep neural networks.\n",
        "- It **maintains**:\n",
        "  1. A running average of **first moments** of gradients (similar to momentum).\n",
        "  2. A running average of **second moments** of gradients (similar to RMSProp).\n",
        "- These running averages help Adam **adapt the learning rate** for each parameter individually.\n",
        "\n",
        "### Why Use Adam?\n",
        "1. **Fast Convergence**: Adam often converges faster than vanilla stochastic gradient descent (SGD) or simple momentum methods.\n",
        "2. **Robust to Noisy Gradients**: The second moment accumulation (variance estimate) helps dampen updates that are too large.\n",
        "3. **Less Hyperparameter Tuning**: Default hyperparameters (e.g., $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-8}\\$ work well for a wide range of problems.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Mathematical Formulation\n",
        "\n",
        "Let $ \\theta $ represent your model parameters (weights). At each time step $ t $:\n",
        "1. **Compute the gradient** of the loss function $ J(\\theta) $ w.r.t. $ \\theta $:  \n",
        "   $\n",
        "   g_t = \\nabla_\\theta J(\\theta_t)\n",
        "   $\n",
        "\n",
        "2. **Update biased first moment estimate** (exponential moving average of the gradients):  \n",
        "   $\n",
        "   m_t = \\beta_1 \\, m_{t-1} + (1 - \\beta_1)\\, g_t\n",
        "   $\n",
        "   - $ m_t $ is the first moment (mean of the gradient).  \n",
        "   - $ \\beta_1 $ (often 0.9) controls the *momentum* or how much past gradients matter.\n",
        "\n",
        "3. **Update biased second moment estimate** (exponential moving average of the squared gradients):  \n",
        "   $\n",
        "   v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, g_t^2\n",
        "   $\n",
        "   - $ v_t $ is the second moment (uncentered variance of the gradient).\n",
        "   - $ \\beta_2 $) (often 0.999) controls how quickly you forget past gradient magnitudes.\n",
        "\n",
        "4. **Bias-corrected estimates**  \n",
        "   Since $ m_t $ and $ v_t $ are **initialized as 0** and are thus biased towards zero, Adam includes correction terms:\n",
        "   $\n",
        "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
        "   \\quad,\\quad\n",
        "   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
        "   $\n",
        "   This step normalizes for the fact that the moving averages start at zero before they have accumulated enough data.\n",
        "\n",
        "5. **Update parameters**  \n",
        "   Finally, Adam updates the parameters by scaling the gradient with the ratio of the first moment to the square root of the second moment (plus a small $\\epsilon$ to avoid division by zero):\n",
        "   $\n",
        "   \\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "   $\n",
        "   - $ \\alpha $ is the **learning rate** (default often $10^{-3}$).  \n",
        "   - $ \\epsilon $ is a small constant (e.g., $10^{-8}$) that prevents division by zero.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Hyperparameters\n",
        "\n",
        "1. **$\\alpha$ (Learning Rate)**  \n",
        "   - Typical default: $10^{-3}$.  \n",
        "   - Sometimes lowered for very sensitive tasks (like $10^{-4}$) or increased if training is too slow (like $10^{-2}$).\n",
        "\n",
        "2. **$\\beta_1$**  \n",
        "   - Controls the exponential decay rate for the first moment.  \n",
        "   - Default $= 0.9$.  \n",
        "   - A higher $\\beta_1$ means more “momentum” from past gradients.\n",
        "\n",
        "3. **$\\beta_2$**  \n",
        "   - Controls the exponential decay rate for the second moment.  \n",
        "   - Default $= 0.999$.  \n",
        "   - A higher $\\beta_2$ means smoother (less noisy) estimates of gradient variance but slower adaptation.\n",
        "\n",
        "4. **$\\epsilon$**  \n",
        "   - A small constant to prevent division by zero (e.g., $10^{-8}$).  \n",
        "   - Usually doesn’t need changing, but can matter if the loss scale is unusual.\n",
        "\n",
        "These defaults often “just work” for many problems, which is one reason Adam is so popular.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Connection to Momentum and RMSProp\n",
        "\n",
        "1. **Momentum**  \n",
        "   - Maintains a velocity vector in parameter space, smoothing out gradient updates over time.  \n",
        "   - Adam’s **first moment** estimate ($ m_t $) is essentially momentum.\n",
        "\n",
        "2. **RMSProp**  \n",
        "   - Keeps an exponentially decaying average of squared gradients.  \n",
        "   - Adam’s **second moment** estimate ($ v_t $) is basically an RMSProp variant.\n",
        "\n",
        "3. **Adam** is best of both worlds:  \n",
        "   - Momentum (stabilizes direction of updates)  \n",
        "   - RMSProp (adapts learning rate to history of gradient magnitudes)\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Practical Tips\n",
        "\n",
        "1. **Default Hyperparameters**  \n",
        "   - Start with $\\alpha = 10^{-3}$, $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-8}$.  \n",
        "   - Adjust only if your loss does not decrease or if you see erratic training behavior.\n",
        "\n",
        "2. **Learning Rate Schedules**  \n",
        "   - Even with Adam, you can still **decay the learning rate** over epochs (e.g., reduce $\\alpha$ by 0.1 after certain epochs or use a scheduler like **Exponential Decay**, **Step Decay**, etc.).\n",
        "\n",
        "3. **Weight Decay / Regularization**  \n",
        "   - Adding weight decay (L2 regularization) can help generalization.  \n",
        "   - Some frameworks have a separate parameter for weight decay. Alternatively, you can manually add an L2 penalty to the gradient.\n",
        "\n",
        "4. **Warm Restarts** (Adam Restarts / SGDR)  \n",
        "   - Sometimes a cyclical or restart-based schedule can help escape local minima or saddle points in deep neural network training.\n",
        "\n",
        "5. **A Note on $\\beta_1 \\approx 1.0$**  \n",
        "   - If $\\beta_1$ is set very close to 1, the bias-corrected moment estimate could become large early on, causing instability. Usually 0.9 or 0.95 is a good balance.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Example Pseudocode\n",
        "\n",
        "Below is simplified pseudocode to illustrate one iteration of Adam:\n",
        "\n",
        "```python\n",
        "# Suppose we have parameters theta, gradients g_t at time t\n",
        "# And we keep track of:\n",
        "#   m, v (initially 0)\n",
        "#   beta1, beta2, alpha (lr), epsilon\n",
        "\n",
        "t = t + 1  # increment timestep\n",
        "\n",
        "# Update biased first moment estimate\n",
        "m = beta1 * m + (1 - beta1) * g_t\n",
        "\n",
        "# Update biased second moment estimate\n",
        "v = beta2 * v + (1 - beta2) * (g_t ** 2)\n",
        "\n",
        "# Compute bias-corrected estimates\n",
        "m_hat = m / (1 - beta1**t)\n",
        "v_hat = v / (1 - beta2**t)\n",
        "\n",
        "# Update parameters\n",
        "theta = theta - alpha * m_hat / (sqrt(v_hat) + epsilon)\n",
        "```\n",
        "\n",
        "In real-world code (TensorFlow, PyTorch, etc.), this update logic is handled by the optimizer class once you specify your chosen hyperparameters.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Advantages and Disadvantages\n",
        "\n",
        "### Advantages\n",
        "1. **Adaptive Learning Rates**: Different parameters can have different effective learning rates, handling sparse gradients better.\n",
        "2. **Fast Convergence**: Often outperforms simpler methods like SGD with momentum in early training.\n",
        "3. **Easy to Use**: The default hyperparameters work well for many problems.\n",
        "\n",
        "### Disadvantages\n",
        "1. **Generalization**: Some research suggests Adam solutions may generalize less robustly than vanilla SGD, especially in certain tasks.  \n",
        "2. **Can Get Stuck**: Like any optimizer, it can stall if poorly tuned or if $\\beta_1$, $\\beta_2$, or $\\alpha$ are chosen badly.  \n",
        "3. **Over-Reliance on Defaults**: While they often work, sometimes careful tuning is still necessary for best results (especially on large or unusual models).\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Summary\n",
        "\n",
        "Adam is a **first-order gradient-based** optimization algorithm that uses estimates of **first** and **second moments** of the gradients to adapt each parameter’s learning rate during training. In practice, it’s one of the most popular optimizers due to its:\n",
        "\n",
        "- **Efficiency** in high-dimensional spaces (common in deep learning).  \n",
        "- **Robustness** to noisy or sparse gradients.  \n",
        "- **Relatively minimal hyperparameter tuning** requirements (compared to other methods).\n",
        "\n",
        "**Key Points**:\n",
        "\n",
        "- **Update Rule**:  \n",
        "  $\n",
        "    m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t, \\quad\n",
        "    v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2\n",
        "  $  \n",
        "  $\n",
        "    \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n",
        "    \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}, \\quad\n",
        "    \\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "  $\n",
        "- **Default Settings** often work well: $\\alpha = 10^{-3}$, $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-8}$.\n",
        "\n",
        "By understanding these details, you can **tune** or **diagnose** Adam more effectively in your own machine learning or deep learning projects."
      ],
      "metadata": {
        "id": "2nTBMLz7BB5k"
      }
    }
  ]
}